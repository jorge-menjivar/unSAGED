{
  "cancel": "Cancel",
  "chat": {
    "creative": "Creative",
    "enterMessage": "Please enter am message",
    "error": "Sorry, there was an error.",
    "maxTokens": "Max Tokens",
    "maxTokensDescription": "The maximum number of tokens to generate. The higher the number, the longer the AI will take to generate a response.",
    "messageLimit": "Message limit is {{maxLength}} characters. You have entered {{valueLength}} characters.",
    "neutral": "Neutral",
    "newConversation": "New conversation",
    "newFolder": "New folder",
    "noData": "No data",
    "precise": "Precise",
    "presencePenalty": "Repeat Penalty",
    "presencePenaltyDescription": "Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Defaults to model provider configuration.",
    "regenerateResponse": "Regenerate response",
    "repeatPenalty": "Repeat Penalty",
    "repeatPenaltyDescription": "Positive values penalize new tokens based on their existing frequency in the text so far. Decreases the model's likelihood to repeat the same line verbatim. Also known as 'frequency penalty'. Defaults to model provider configuration.",
    "search": "Search",
    "seed": "Seed",
    "seedDescription": "The seed used to generate the response. The same seed will always generate the same response.",
    "startTyping": "Start typing, type / to select a template...",
    "stop": "Stop",
    "stopDescription": "Comma-separated list of tokens to stop generation on. The AI will stop generating tokens once it encounters any of these tokens.",
    "stopGenerating": "Stop generating",
    "temperature": "Temperature",
    "temperatureDescription": "Higher values will make the output more random, while lower values will make it more focused and deterministic.",
    "topK": "Top K",
    "topKDescription": "The number of highest probability vocabulary tokens to keep for top-k-filtering. Between 1 and infinity. Defaults to model provider configuration.",
    "topP": "Top P",
    "topPDescription": "The cumulative probability of parameter highest probability tokens to use for nucleus sampling, between 0 and 1. Defaults to model provider configuration."
  },
  "clearConversations": "Clear conversations",
  "error": {
    "fetchingModels": "Error fetching models.",
    "keyMissing": "Make sure your {{vendor}} key is set in the bottom left of the sidebar.",
    "vendorIssue": "If you completed this step, {{vendor}} may be experiencing issues."
  },
  "loading": "Loading",
  "markdown": {
    "copied": "Copied!",
    "copyCode": "Copy Code",
    "enterFileName": "Enter file name"
  },
  "model": "Model",
  "modelDescription": "The model used for this conversation",
  "models": "Models",
  "name": "Name",
  "newSystemPrompt": "New system prompt",
  "noData": "No data",
  "prompt": "Prompt",
  "promptbar": {
    "description": "Description",
    "name": "Name",
    "newFolder": "New folder",
    "newMessageTemplate": "New message template",
    "prompt": "Prompt",
    "promptContent": "Prompt content. Use {{}} to denote a variable. Ex: {{name}} is a {{adjective}} {{noun}}",
    "save": "Save"
  },
  "save": "Save",
  "saveSubmit": "Save & Submit",
  "search": "Search",
  "sidebar": {
    "exportData": "Export Data",
    "importData": "Import Data"
  },
  "submit": "Submit",
  "sureQ": "Are you sure?",
  "systemPrompt": "System prompt",
  "systemPromptDescription": "The system prompt to use when sending a message"
}