{
  "search": "Search",
  "loading": "Loading",
  "noData": "No data",
  "cancel": "Cancel",
  "save": "Save",
  "saveSubmit": "Save & Submit",
  "submit": "Submit",
  "sureQ": "Are you sure?",
  "clearConversations": "Clear conversations",
  "newSystemPrompt": "New system prompt",
  "name": "Name",
  "prompt": "Prompt",
  "systemPrompt": "System prompt",
  "systemPromptDescription": "The system prompt to use when sending a message",
  "model": "Model",
  "modelDescription": "The model used for this conversation",
  "models": "Models",
  "chat": {
    "newConversation": "New conversation",
    "newFolder": "New folder",
    "search": "Search",
    "noData": "No data",
    "enterMessage": "Please enter am message",
    "messageLimit": "Message limit is {{maxLength}} characters. You have entered {{valueLength}} characters.",
    "stopGenerating": "Stop generating",
    "regenerateResponse": "Regenerate response",
    "startTyping": "Start typing, type / to select a template...",
    "error": "Sorry, there was an error.",
    "repeatPenalty": "Repeat Penalty",
    "repeatPenaltyDescription": "Positive values penalize new tokens based on their existing frequency in the text so far. Decreases the model's likelihood to repeat the same line verbatim. Also known as 'frequency penalty'. Defaults to model provider configuration.",
    "presencePenalty": "Repeat Penalty",
    "presencePenaltyDescription": "Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Defaults to model provider configuration.",
    "maxTokens": "Max Tokens",
    "maxTokensDescription": "The maximum number of tokens to generate. The higher the number, the longer the AI will take to generate a response.",
    "seed": "Seed",
    "seedDescription": "The seed used to generate the response. The same seed will always generate the same response.",
    "stop": "Stop",
    "stopDescription": "Comma-separated list of tokens to stop generation on. The AI will stop generating tokens once it encounters any of these tokens.",
    "temperature": "Temperature",
    "temperatureDescription": "Higher values will make the output more random, while lower values will make it more focused and deterministic.",
    "precise": "Precise",
    "neutral": "Neutral",
    "creative": "Creative",
    "topK": "Top K",
    "topKDescription": "The number of highest probability vocabulary tokens to keep for top-k-filtering. Between 1 and infinity. Defaults to model provider configuration.",
    "topP": "Top P",
    "topPDescription": "The cumulative probability of parameter highest probability tokens to use for nucleus sampling, between 0 and 1. Defaults to model provider configuration."
  },
  "markdown": {
    "enterFileName": "Enter file name",
    "copied": "Copied!",
    "copyCode": "Copy Code"
  },
  "promptbar": {
    "newFolder": "New folder",
    "newMessageTemplate": "New message template",
    "name": "Name",
    "description": "Description",
    "prompt": "Prompt",
    "promptContent": "Prompt content. Use {{}} to denote a variable. Ex: {{name}} is a {{adjective}} {{noun}}",
    "save": "Save"
  },
  "sidebar": {
    "importData": "Import Data",
    "exportData": "Export Data"
  },
  "error": {
    "fetchingModels": "Error fetching models.",
    "keyMissing": "Make sure your {{vendor}} key is set in the bottom left of the sidebar.",
    "vendorIssue": "If you completed this step, {{vendor}} may be experiencing issues."
  }
}
