{
  "advancedSettings": "Erweiterte Einstellungen",
  "cancel": "Abbrechen",
  "chat": {
    "creative": "Kreativ",
    "enterMessage": "Bitte gib eine Nachricht ein",
    "error": "Entschuldigung, es ist ein Fehler aufgetreten.",
    "maxTokens": "Max Tokens",
    "maxTokensDescription": "Die maximale Anzahl von zu generierenden Tokens. Je höher die Zahl, desto länger dauert es, bis die KI eine Antwort generiert.",
    "messageLimit": "Das Nachrichtenlimit beträgt {{maxLength}} Zeichen. Du hast bereits {{valueLength}} Zeichen eingegeben.",
    "neutral": "Neutral",
    "newConversation": "Neue Konversation",
    "newFolder": "Neuer Ordner",
    "precise": "Präzise",
    "presencePenalty": "Repeat Penalty",
    "presencePenaltyDescription": "Positive Werte bestrafen neue Tokens, je nachdem, ob sie bisher im Text vorkommen, und erhöhen so die Wahrscheinlichkeit, dass das Modell über neue Themen spricht. Die Standardeinstellung ist die Konfiguration des Model-Anbieters.",
    "regenerateResponse": "Antwort erneut generieren",
    "repeatPenalty": "Repeat Penalty",
    "repeatPenaltyDescription": "Positive Werte bestrafen neue Tokens aufgrund ihrer bisherigen Häufigkeit im Text. Verringert die Wahrscheinlichkeit, dass das Modell dieselbe Zeile wörtlich wiederholt. Wird auch als „frequency penalty“ bezeichnet. Standardmäßig wird die Konfiguration des Model-Anbieters verwendet.",
    "search": "Suche",
    "seed": "Seed",
    "seedDescription": "Der Seed, der zur Generierung der Antwort verwendet wurde. Derselbe Seed generiert immer dieselbe Antwort.",
    "startTyping": "Starte zu schreiben, schreibe / um eine Template auszuwählen...",
    "stop": "Stop",
    "stopDescription": "Kommagetrennte Liste von Tokens, für die die Generierung beendet werden soll. Die KI hört auf, Tokens zu generieren, sobald sie auf eines dieser Tokens trifft.",
    "stopGenerating": "Generieren stoppen",
    "temperature": "Temperature",
    "temperatureDescription": "Höhere Werte machen die Ausgabe zufälliger, während niedrigere Werte sie fokussierter und deterministischer machen.",
    "topK": "Top K",
    "topKDescription": "Die Anzahl der Token mit der höchsten Wahrscheinlichkeit, die für die Top-K-Filterung aufbewahrt werden müssen. Zwischen 1 und unendlich. Standardmäßig wird die Konfiguration des Model-Anbieters verwendet.",
    "topP": "Top P",
    "topPDescription": "Die kumulative Wahrscheinlichkeit, dass Tokens mit der höchsten Wahrscheinlichkeit von Parametern für die Kernprobenahme verwendet werden, zwischen 0 und 1. Die Standardeinstellung ist die Konfiguration des Model-Anbieters."
  },
  "clearConversations": "Alle Konversationen löschen",
  "default": "Standard",
  "error": {
    "fetchingModels": "Fehler beim Abrufen der Sprachmodelle.",
    "keyMissing": "Stelle sicher, dass dein {{vendor}} Schlüssel in der unteren linken Ecke der Seitenleiste eingetragen ist.",
    "vendorIssue": "Wenn dies der Fall ist, könnte {{vendor}} möglicherweise momentan Probleme haben."
  },
  "loading": "Laden",
  "markdown": {
    "copied": "Kopiert!",
    "copyCode": "Code kopieren",
    "enterFileName": "Dateinamen eingeben"
  },
  "model": "Modell",
  "modelDescription": "Das Modell was für diese Konversation genutzt wird",
  "models": "Modelle",
  "name": "Name",
  "newSystemPrompt": "Neue Systemaufforderung",
  "noData": "Keine Daten",
  "prompt": "Prompt",
  "promptbar": {
    "description": "Beschreibung",
    "name": "Name",
    "newFolder": "Neuer Ordner",
    "newMessageTemplate": "Neue Vorlage",
    "prompt": "Prompt",
    "promptContent": "Prompt Inhalt. {{}} werden verwendet, um eine Variable zu bezeichnen. Beispiel: {{name}} ist ein {{adjective}} {{noun}}",
    "save": "Speichern"
  },
  "save": "Speichern",
  "saveSubmit": "Speichern & Absenden",
  "search": "Suche",
  "settings": "Einstellungen",
  "sidebar": {
    "exportData": "Daten exportieren",
    "importData": "Daten importieren"
  },
  "submit": "Absenden",
  "sureQ": "Bist du sicher?",
  "systemPrompt": "Systemaufforderung",
  "systemPromptDescription": "Die Aufforderung die bei jeder Nachricht mitgesendet wird"
}